# ARAM_new
# ARMA: Mitigating Catastrophic Forgetting using Attention-Regularized Model Averaging in Continual Fine-tuning Large Language Models

## Introduction

Welcome to the repository for **ARMA**, a novel framework designed to address catastrophic forgetting in large language models (LLMs) during continual fine-tuning. Catastrophic forgetting is a common challenge when fine-tuning models on domain-specific tasks, often leading to a decline in performance on previously learned tasks. ARMA mitigates this issue by utilizing attention-regularized model averaging, which balances the trade-off between domain-specific improvements and general task performance.

## Features

- **Attention-Regularized Model Averaging**: Uses attention distribution to maintain general task performance while fine-tuning for domain-specific tasks.
- **Reinforcement Learning Optimization**: Employs reinforcement learning to find the optimal balance between the base model and the fine-tuned model, avoiding exhaustive parameter searches.
- **General Task Preservation**: Ensures that the model retains its ability to perform well on general tasks even after domain-specific fine-tuning.

## Installation

The code is currently being organized and will be uploaded soon. Please stay tuned for updates.

## Usage

### Training the Model

Detailed instructions for preparing your dataset and running the training script will be provided once the code is uploaded.

### Evaluation

Instructions for evaluating the model's performance on various benchmarks will be made available with the code release.

### Hyperparameter Tuning

Guidelines for exploring different configurations and tuning hyperparameters will be provided in future updates.

### Experiment Reproduction

Steps to reproduce the experiments from the paper will be shared once the code is uploaded.

## Results

The experimental results show that ARMA significantly reduces catastrophic forgetting while maintaining or even improving performance on general tasks. More detailed results and performance metrics will be included with the code release.

## Contributing

Contributions are welcome! More details on how to contribute will be added once the repository is fully set up.

## License

This project is licensed under the MIT License. The `LICENSE.md` file will be added soon.

## Acknowledgments


For any issues or questions, please open an issue in this repository or contact the maintainers directly.

Happy experimenting!
